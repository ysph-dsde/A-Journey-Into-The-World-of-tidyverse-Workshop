---
  title: "Worked Through Example"
  authors: Shelby Golden, M.S. and Howard Baik, M.S.
  format: html
  editor: visual
  markdown: 
  editor_options: 
  chunk_output_type: console
---

## Introduction

In this workshop, we will explore the **tidyverse** collection of R packages to clean, analyze, and visualize COVID-19 daily death counts in the United States. Additionally, we will make our visualizations interactive using the `plotly` package.

## Setup

First, we will load the necessary libraries and any special functions used in the script.

```{r message = FALSE, warning = FALSE}
library(readr)      # For reading in the data
library(tidyr)      # For tidying data 
library(dplyr)      # For data manipulation 
library(stringr)    # For string manipulation
library(lubridate)  # For date manipulation
library(ggplot2)    # For creating static visualizations
library(plotly)     # For interactive plots
library(scales)     # For formatting plots axis


# Function to select "Not In"
'%!in%' <- function(x,y)!('%in%'(x,y))
```

Now we will fetch the semi-cleaned COVID-19 deaths data set that has been prepared for this example. The original, raw data comes from the Johns Hopkins University Coronavirus Resource Center (JHU CRC) Center for Systems Science and Engineering (CSSE) GitHub page: CSSEGISandData. The full data cleaning script and links to the original raw data set can be found in the GitHub page for this workshop.

```{r}
# Save the GitHub raw URL associated with our data set.
df_url  <- "https://raw.githubusercontent.com/ysph-dsde/A-Journey-Into-The-World-of-tidyverse-Workshop/refs/heads/main/Data%20for%20the%20Workshop_Aggregated%20by%20Month.csv"

# Read in the data set using the URL via the readr package.
df_raw  <- read_csv(file = df_url, show_col_types = FALSE)
```

It's good practice to review aspects of your raw data before proceeding with cleaning it. This includes:\
- Reviewing the variable data dictionary.\
- Looking at the dimensions of the data set.\
- Looking at the first and/or last few rows.\
- Looking at the variable classification for each column of information.

```{r results = "hold", tidy = FALSE}
# Take a look at the first few rows and columns of the raw data.
head(df_raw)
```

We see that the data set has `r nrow(df_raw)` unique regions in the U.S. at the county- and state-level with `r ncol(df_raw)` columns of information. The first four columns denotes the region in different formats, and the remaining 39 columns represent the date when COVID-19 deaths were added to the running tally. All counts are reported as cumulative (monotonically increasing), and have been summarized to monthly updates.

In following sections we will continue reviewing aspects of our data set, including confirming the variable classes are correct. Before we continue, we need to tidy our data, which is done in the next section.

## Data tidying using tidyr

As noted in the previous section, observations are added as new columns, creating what is referred to as a wide data set. To render the raw data into the target "tidy" format we need columns to only represent variables. This can be achieved by transforming the array of dates into new rows instead of columns, referred to as a long data set.

`tidyr` provides us with a convenient function that will do such a transformation in one expression, `pivot_longer()`. In that function, we need to indicate which array of columns need to be transposed from columns to rows.

```{r}
# Find which dates constitute the start and end of our array.
str_c(c("Start date: ", "End date: "), colnames(df_raw)[c(5, ncol(df_raw))])
```

Using the start and end column names of the dates array, we can pivot the raw data into a long-format.

Notice that we also need to provide the names of two new columns generated by this operation: one column to store the pivoted column names and one column to store the values previously expanded under those columns. Also notice that the colon in `"2020-01-01":"2023-03-01"` is prompting the function to use all consecutive columns between the first and last column name denoted.

```{r}
df_long <- df_raw |> 
  # Step 1: Convert wide-format date columns to long-format.
  pivot_longer(
    # Designate which columns need to be pivoted.
    cols = "2020-01-01":"2023-03-01", 
    # Name the variable that will store newly pivoted column names.
    names_to = "Date",
    # Name the variable that will store respective cell values.
    values_to = "Deaths_Count_Cumulative"
  ) |> 
  # Step 2: Change table format to "data frame" class for convenience.
  as.data.frame()

# Inspect the pivoted data.
head(df_long)
```

```{r echo = FALSE}
# Report the dimensions of the pivoted data set.
str_c(c("Number of rows: ", "Number of columns: "), c(nrow(df_long), ncol(df_long)) )
```

This formatting looks quite a lot better. We see that each column represents its own variable, and each row represents a new observation over a range of dates for each county- and state-level region. Technically, our data set is now "tidy", but might still require some additional preparations or cleaning.

In following sections we will continue reviewing and processing the data so that it is ready for visualization and statistical analyses.

## Data manipulation using dplyr

Let's say we want to plot deaths as a result of the COVID-19 pandemic for the whole U.S. We will need to generate these counts from the data set we have. This can be accomplished by summing cumulative deaths counts grouped by region (`Combined_Key`) and the month when new deaths were reported (`Date`). Cumulative counts are difficult to interpret, and so we will also want to back-calculate the monthly counts.

`dplyr` provides us with a series of functions that allow us to do such data manipulation tasks. Over the next four code blocks, we will show how to generate counts for the whole U.S.

### Calculate total U.S. counts

Recall that the imported data set includes values for U.S. counties, states, and territories. We need to calculate U.S. values over the U.S. states, including the District of Columbia, only. This can be achieved by first filtering for `Combined_Key` entries that match those state names using `filter()`.

Notice that we are using a `stringr` function, `str_c()` or string concatenate, to create a vector of names that can be matched with entries in `Combined_Key`. This function will be discussed in the next section covering `stringr`.

**Step 1:** Filter subsets the data set by rows that match the condition.

```{r}
df_filtered <- df_long |> 
  filter(Combined_Key %in% str_c(c(datasets::state.name, "District of Columbia"), ", US"))
```

The full data set has `r nrow(df_long)` rows and after filtering we only have `r nrow(df_filtered)` remaining. These each represent the observations for all U.S. states, including the District of Columbia which was reported separately. Recall that there were 39 months within the span of the pandemic where observations were recorded, meaning we have `r nrow(df_long)`/39 = 51 unique regions reported.

Using this filtered data set, we will want to group row entries by unique months. We accomplish that using `group_by()`. In the table that follows this grouping, we show how many rows are being grouped with each unique month using `tally()`.

**Step 2:** Groups the table by unique entries in `Date`.

```{r}
df_grouped <- df_filtered |>
  group_by(Date)

# Display the number of entries associated with each group using tally().
df_grouped |> tally() |> head()
```

As expected, we have 51 row entries that are being grouped by each month recorded. Now we are ready to calculate the total U.S. counts. The function `summarize()` allows us to iterate a function over each grouping. If necessary, the function can be prompted to retain previously designated groups using `.groups = "keep"`.

**Step 3:** Calculate the cumulative deaths variable for the U.S. by summing over the grouped rows.

```{r}
df_US <- df_grouped |>
  summarise(Deaths_Count_Cumulative = sum(Deaths_Count_Cumulative), .groups = "keep")
```

Finally, we need to remove the groupings so we can work with the whole data set again.

**Step 4:** Remove grouping to work with the full data frame again.

```{r}
df_US <- df_US |>
  ungroup()
```

After these four operations, we get the following data set:

```{r}
head(df_US)
```

Now we are ready to merge this back with the main data frame. `bind_rows()` is a tidyverse function that is similar to `do.call()`, but it will also fill missing columns with `NA` so that the maximum number of uniquely defined variables are maintained.

Right now, the data frame `df_US` is missing the `Combined_Key` column. If we use `bind_rows()` at this stage, then `Combined_Key` for all U.S. rows will be `NA`. Instead, we want to add a new `Combined_Key` that says `"US"`. This is achieved using `mutate()`, which generates a new column using a functions of existing columns or static operations.

```{r}
df_US <- df_US |>
  mutate(Combined_Key = "US")

# Inspect the mutated data.
head(df_US)
```

Great, now that we have all the columns we need, we are ready to merge it with `df_long`, which contains all of the county- and state-level data.

```{r}
df <- bind_rows(df_US, df_long) |> 
  # Change table format to "data frame" class for convenience.
  as.data.frame()

# Inspect the merged data.
head(df)
tail(df)
```

The `select()` function is used to keep or drop columns. It can also be used to organize the existing columns. We would like to organize the columns by region followed by the month the record was made and then the cumulative deaths recorded.

```{r}
df <- df |>
  # Generate new columns using mutate.
  select(Combined_Key, County, Province_State, Country_Region, Date, Deaths_Count_Cumulative)

# Inspect the merged data.
head(df)
```

### Backcalculate monthly updates

Now we are ready to calculate the monthly death counts from the cumulative counts. Earlier we implied that `mutate()` can be used to generate new columns that are functions of existing columns. Here, we generate a new vector using `diff()` to back-calculated values added each month.

```{r}
# Calculate daily monthly deaths counts using mutate.
df <- df |>  
  mutate(Deaths_Count_Monthly = c(Deaths_Count_Cumulative[1], diff(Deaths_Count_Cumulative)))

# Inspect the wrangled data set.
head(df)
```

As a last step, we will inspect the classifications for each variable and ensure that they are correctly classified.

```{r}
sapply(df, class)
```

Currently, `Date` is classified as a character. We can change the class to date using the lubridate package from tidyverse. This package is not covered in this introductory workshop, but those interested to find out more can review the package documentation: https://lubridate.tidyverse.org/

Notice that the data has been reported in yyyy/mm/dd format. Therefore, we use `ymd()` to correctly convert the character to the date class. We also format these to only report the year followed by the month abbreviation.

```{r}
df$Date <- ymd(df$Date)
```

## String manipulation using stringr

After completing bulk data wrangling operations, like the ones completed above, it is good practice to examine variable classifications and nomenclature before any calculations. For example, if you have a variable for the sex of a participant, you will want to confirm that all entries of that variable say `"M"` and `"F"` for Male and Female, respectively. You might also need to confirm that zero's are not being used in place of `NA` when the value is not determined.

In this scenario, we are going to assume that the county-level entries are correct. We expect that the U.S. states and territories are represented for state-level entries. We can examine other state-level entries by reporting the complement of matches to unique entries of `Combined_Key` to `datasets::state.name` and the District of Columbia.

```{r}
# Find the non-states represented in the state-level data set.
unique(df$Province_State)[unique(df$Province_State) %!in% c(datasets::state.name, "District of Columbia")]
```

Notice that there are `NA` detected. This should be correct, since the country-level counts are `NA` at the state-level. We can confirm this by showing all `Combined_Key` entries are "US" for rows with `"Province_State" = NA`.

```{r}
df[df$Province_State %in% NA, "Combined_Key"] |> unique()
```

In addition to the five U.S. territories, there are entries for two cruise ships. These are not relevant to our analysis, and so we exclude them using `str_detect()` or `str_which()`. Both of these functions will find the rows where a string pattern is matched, and report the findings as either a Boolean (`TRUE`/`FALSE`) or row-index.

**Option #1:** Use the Boolean test that detects the `"Princess"` string. Note that the added `%in% TRUE` is used to address NA outcomes that need to be interpreted as `FALSE`.

```{r}
df_filtered <- df[!str_detect(df$Province_State, "Princess") %in% TRUE, ]
```

**Option #2:** Find the index that detects the "Princess" string and use the indexes that do not contain that string to subset.

```{r}
df_filtered <- df[-str_which(df$Province_State, "Princess"), ]
```

Notice one could use `str_which(df$Combined_Key, "Princess", negate = TRUE)`. However, because there are `NA` in the "Combined_Key" vector the version displayed above needs to be used to maintain those rows.

Doing this removes the following number of rows from the larger data set; this translates to two unique regions with 39 dates associated with each, as expected.

```{r}
nrow(df) - nrow(df_filtered)
```

The `Combined_Key` variable combines information from `County`, `Province_State`, and `Country_Region`. Assume we only have the county-level data and wish to generate the state- and country-level `Combined_Key` variable. We can do this in two ways: `str_c()` or `str_split()`.

For this example we will subset the data by county-level information only. We use `str_count()` to represent the number of times a string pattern is detected within any given string. County-level data will have two commas, so this is one method we can use to subset our data.

```{r}
df_county <- df_filtered[str_count(df_filtered$Combined_Key, ",") == 2, ]
```

**Option #1:** Generate a new column by combining the desired columns with `", "` as the separator.

```{r}
str_c(df_county$Province_State, df_county$Country_Region, sep = ", ") |> unique()
```

**Option #2:** Split the string only to the first observation of the string match.

```{r}
str_split(df_county$Combined_Key, ",", simplify = TRUE, n = 2)[, 2] |> 
  str_trim(side = "both") |> unique()
```

Say we wish to specify that the Virgin Islands entries only reflect results for the U.S. territory. We can replace specific strings exactly using `str_replace()`.

Going forward, we only require the `Combined_Key` variable, as the `Country_Region`, `Province_State`, and `County` variables are succinctly represented there. We can adjust the `"Virgin Islands"` entries for that column only.

```{r}
df_filtered[, "Combined_Key"] <- str_replace(df_filtered[, "Combined_Key"],  "Virgin Islands", "US Virgin Islands")
```

### Bonus: Iterating Operations Over Columns

Note that it is possible to adjust multiple columns at once using `sapply()`. Notice that with the base pipe `|>` the standard placeholder `_` does not move information into the `sapply()` function. To fix this, we wrap the `sapply()` pipe level and specify a placeholder for the values being passed from the left-side to `sapply()`. In this scenario, we are calling that information `x`.

`sapply()` is one of a few useful functions that repeats operations over columns, rows, or lists. `sapply()` will only apply the function over a the columns of a data frame.

```{r}
# Identify the row indices where "Virgin Islands" is detected in "Province_State".
index = str_which(df_filtered[, c("Province_State")], "Virgin Islands")

df_filtered[index, c("Province_State", "Combined_Key")] |>
  # Define the wrapper of this pipe-level and specify the information from
  # the left to be "x".
  (\(x) {
    # Apply the str_replace() function over "Province_State" and "Combined_Key".
    # Notice that the added "^ " is a regex statement that defines a hard
    # boundary on the string.
    sapply(x, function(y) str_replace(y,  "^Virgin Islands", "US Virgin Islands"))
  }) () |> 
  # Show that the changes have been completed.
  _[1:15, ]
```

## Visualization: Daily COVID-19 Death Counts

We will calculate the daily COVID-19 death counts across the U.S. and create a line plot showing the daily COVID-19 death counts in the U.S.

```{r}

plot_deaths <- df |>
  filter(Combined_Key == "US") |>
  # Rename column names so they look nicer in plotly
  rename(Date = Date, Count = Deaths_Count_Monthly) |>  
  ggplot(aes(x = Date, y = Count)) +
  geom_line(aes(x = Date, y = Count), color = "#880808", size = 1) +
  # Format the y-axis to show values in terms of thousands.
  scale_y_continuous(labels = unit_format(unit = "K", scale = 1e-3)) +
  # Format the x-axis to show dates as Jan 2020 from 01/01/2020, spaced
  # every four months.
  scale_x_date(date_breaks = "4 month", date_labels =  "%b %Y") +
  labs(x = NULL, y = "Daily Counts",
       title = "Daily Death Counts of COVID-19 in the US") +
  theme_minimal()

# Make plot interactive
ggplotly(plot_deaths)
```

## Challenge Questions

1.  Filter the `df` data set to include only rows from 2021.
2.  With the data set filtered for rows from 2021, determine the day with the highest death count along with the corresponding count.
